{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b372017-53b0-4a20-afdc-b97adac7fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c14426-fc34-4095-9e6f-82fd8c75b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42863cc1-2cce-4e73-b63b-107c4fcdc672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     9136\n",
       "INLAND        6551\n",
       "NEAR OCEAN    2658\n",
       "NEAR BAY      2290\n",
       "ISLAND           5\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ocean_proximity'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7667b3f4-d25e-41b9-8ceb-b6d8d6e7aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b0373b4-3a32-4470-8ac1-3410eb5e0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\", \n",
    "    \"median_income\",\n",
    "    \"median_house_value\"\n",
    "]\n",
    "\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91db656e-4b6c-41c3-b4a7-46fe0098fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "longitude               0\n",
       "latitude                0\n",
       "housing_median_age      0\n",
       "total_rooms             0\n",
       "total_bedrooms        157\n",
       "population              0\n",
       "households              0\n",
       "median_income           0\n",
       "median_house_value      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6719e4f-2077-49c6-8743-a75a673dc612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['population'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc93f72-6f9a-4292-be93-57013a730253",
   "metadata": {
    "tags": []
   },
   "source": [
    "#So here I am taking the length of the entire dataframe.\n",
    "and naming it N.\n",
    "\n",
    "Then I will be creating the dataframes for validating, testing and training. \n",
    "In order to make sure I do not get decimals, I will be rounding them using the (int) function. \n",
    "the Train dataset is simply the subtraction of val and test from the total n. \n",
    "\n",
    "If I don't do this then the sum of val,test and train will be higher than the original n dataset because they will all be rounded up numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f80102-3b44-45dd-9227-d4de40c51e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "\n",
    "n_val = int(0.2 * n)\n",
    "n_test = int(0.2 * n)\n",
    "n_train = n - (n_val + n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc60a8-f0bc-4bc5-9206-3d373259183c",
   "metadata": {},
   "source": [
    "arange is in essence creating a subset of numbers from 0 to the end of N. Then by using IDX, you can shuffle the range as you like. Seed allows for reproducing the data at a time in the future. So the instructor requested 42 so that we can all have the same results in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31a3155-0a49-4dd7-93da-77491f25b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(n)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b70ace-7ece-40c4-adad-d72a05740581",
   "metadata": {},
   "source": [
    "df.iloc[idx]: This selects rows from df based on the order of indices in idx. Essentially, it's reordering or shuffling the rows of df based on the shuffled indices in idx.\n",
    "The result is a new DataFrame df_shuffled where the rows have been shuffled or reordered based on the shuffled indices in idx.\n",
    "\n",
    "For example, if df has 5 rows and idx is [3, 1, 4, 2, 0], then the first row of df_shuffled will be the fourth row of df, the second row of df_shuffled will be the second row of df, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d127636-d94f-4783-b222-f86df8218150",
   "metadata": {},
   "source": [
    "n_train and val etc are only numbers. To now append the 20,20,60% split to the dataframe we need to use iloc to append the integer match to the df-shuffled dataframe.\n",
    "\n",
    "We end up with an additional index and so the need to reset and drop is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08029adf-e94f-4924-b127-692d59b0f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = df.iloc[idx]\n",
    "\n",
    "df_train = df_shuffled.iloc[:n_train].copy()\n",
    "df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()\n",
    "df_test = df_shuffled.iloc[n_train+n_val:].copy()\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0d0e4-f264-4ab3-91bb-f76e7350c52c",
   "metadata": {},
   "source": [
    "Now we are creating numpy arrays which are essential matrices to the y values, which are the values needed for predictions. However, these are the ACTUAL values, not the predictions. We will be solving for these later. We are also applying logarithm to these values as well and then deleting median_house_value from the actual dataset so as not to throw off the regression model.\n",
    "\n",
    "\n",
    "Lo1pg can reduce the impact of outliers. Which makes sense considering many times the distribution will be long tailed, and long tailed distribution necessitates outliers, sometimes even being 0. Which is why 1p is used, to ensure there are no zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1512a0-f882-4be7-9b18-2baa1a45fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_orig = df_train.median_house_value.values\n",
    "y_val_orig = df_val.median_house_value.values\n",
    "y_test_orig = df_test.median_house_value.values\n",
    "\n",
    "y_train = np.log1p(y_train_orig)\n",
    "y_val = np.log1p(y_val_orig)\n",
    "y_test = np.log1p(y_test_orig)\n",
    "\n",
    "del df_train['median_house_value']\n",
    "del df_val['median_house_value']\n",
    "del df_test['median_house_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a9aa7-aa9a-49b1-9db0-aec2f846395c",
   "metadata": {},
   "source": [
    "Now we are creating the Linear Regression Model.\n",
    "First we are creating a vector of ones which will be the bias term. The bias term is needed in the model and specifically for the feature model so as to give the model variance when computing. My understanding is that without 1's, the model will continue to go through the 0,0 portion of the diagram and lack variance, thus not allowing for variance in predictions.\n",
    "\n",
    "From ChatGPT\n",
    "Bias Term (Intercept):\n",
    "The bias term (often referred to as the intercept) is indeed crucial for the linear regression model. It allows the regression line to not necessarily pass through the origin (0,0). Without the bias term, the regression line would be forced to go through the origin, which might not provide a good fit to the data in many cases.\n",
    "\n",
    "Variance in Predictions:\n",
    "The bias term doesn't directly provide \"variance in predictions\" in the sense of making predictions more diverse or spread out. Instead, it provides an offset. It allows the model to make predictions that are offset from what they would be if the regression line passed through the origin. This can be essential for capturing the underlying patterns in the data.\n",
    "\n",
    "\n",
    "\n",
    "The stack is changing it from an an array to fit the X Matrix that it is now appended to.\n",
    "\n",
    "X.T is the transpose, X is the original dataframe, and T is a numpy attribute that gives you the transpose. Then you multiply it by itself and get the XTX or Gram Matrix which then you solve for the inverse of it, and then multiply the X by the inverse, and multiply the Y and you get the weight.\n",
    "\n",
    "The return is giving me the weight split into the bias term which is w[0] which signifies giving me the index 0 or the first column, and then w[1:] is giving me the first row and everything after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12eeec5-3235-49a9-9650-69e2536c1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X, y):\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "\n",
    "    XTX = X.T.dot(X)\n",
    "    XTX_inv = np.linalg.inv(XTX)\n",
    "    w = XTX_inv.dot(X.T).dot(y)\n",
    "    \n",
    "    return w[0], w[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0d9d8-3c69-47a1-8670-39baaca52332",
   "metadata": {},
   "source": [
    "What's happening here is a function to fill all of the full values in the datatset instead of leaving them blank. This allows for the linear regression model to run. We could also solve for the mean of the null_values but this is how we've chosen here. The df.values = X is simply giving us the numpy.array or the matrix that we will run the regression with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d216ced0-b682-4bdd-b922-1ddd71e38c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X(df, fillna_value):\n",
    "    df = df.fillna(fillna_value)\n",
    "    X = df.values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1a5b0-1fbc-4bb9-a6a6-e4c671b577af",
   "metadata": {},
   "source": [
    "This portion is to solve for the level of error between the prediction and actual target variable.\n",
    "\n",
    "It is squared to ensure the numbers are positive and naturally the largest numbers will be the greatest margins of error. Solving for the mean now gives us a general idea of how far off our model is. The higher the number the worse our model is. Apparently multiplying by 2 is not needed but it is done. \n",
    "\n",
    "Chatgpt\n",
    "\n",
    "Multiplying by 2: Ah, the mysterious multiplication by 2! In the context of gradient descent optimization (a method to tweak the model to reduce error), the derivative of the MSE includes a factor of 2. To simplify calculations, some folks include this factor directly in the MSE. However, for evaluation purposes, this factor is often unnecessary. It's a bit like wearing two watches to ensure punctuality â€“ a tad overzealous, but it has its reasons.\n",
    "\n",
    "Squaring the error ensures that it is positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0308b313-6733-4783-ab80-209462e3d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    error = y_pred - y\n",
    "    mse = (error ** 2).mean()\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461eac62-dad6-4cea-89eb-d7239f7b441b",
   "metadata": {},
   "source": [
    "Mean of Total Bedrooms: The choice to fill missing values in the 'total_bedrooms' feature with its mean is likely based on domain knowledge or exploratory data analysis. \n",
    "Perhaps 'total_bedrooms' has missing values that need addressing, while other features might not have missing values or might be handled differently. \n",
    "Using the mean is a common strategy to fill in missing data because it preserves the overall distribution of the feature.\n",
    "\n",
    "Preparing X_train: Spot on! You're using the prepare_X function to clean the training dataset. By filling in missing values in 'total_bedrooms' with its mean, \n",
    "you're ensuring that the model doesn't get tripped up by any gaps in the data. And yes, the train_linear_regression function is geared towards finding the optimal weights \n",
    "for the model based on the provided data. It's not the entire linear regression algorithm but rather a crucial part of it that determines how each feature influences the prediction.\n",
    "\n",
    "Preparing X_val and Predicting: Absolutely right! You're preparing the validation dataset in the same way as the training dataset, ensuring consistency. With the weights (w_0 and w) obtained from training, you're making predictions on this validation set. The equation y_pred = w_0 + X_val.dot(w) is the linear regression prediction formula in action.\n",
    "\n",
    "Calculating RMSE: Precisely! The rmse function computes the Root Mean Squared Error, a measure of how far off your predictions are from the actual values. By using it here, you're assessing the model's performance on the validation set, especially after addressing the missing values with the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88bd10aa-a786-4591-94bb-461574b49b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34056998014452095"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = df_train.total_bedrooms.mean()\n",
    "\n",
    "X_train = prepare_X(df_train, fillna_value=mean)\n",
    "w_0, w = train_linear_regression(X_train, y_train)\n",
    "\n",
    "X_val = prepare_X(df_val, fillna_value=mean)\n",
    "y_pred = w_0 + X_val.dot(w)\n",
    "\n",
    "rmse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "727b4fa4-5969-4787-ae31-e9123edbd00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34084790341630966"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = prepare_X(df_train, fillna_value=0)\n",
    "w_0, w = train_linear_regression(X_train, y_train)\n",
    "\n",
    "X_val = prepare_X(df_val, fillna_value=0)\n",
    "y_pred = w_0 + X_val.dot(w)\n",
    "\n",
    "rmse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac3e8f0-716e-4b09-91c2-29d5bdba45d9",
   "metadata": {},
   "source": [
    "## Regularization in Linear Regression\n",
    "\n",
    "In our linear regression model, we introduce a **regularization** term. The primary purpose of this term is to mitigate **overfitting**. Overfitting occurs when our regression model performs exceptionally well on the training data but fails to generalize effectively to new, unseen data.\n",
    "\n",
    "### The Role of `np.eye` and Regularization\n",
    "\n",
    "The function `np.eye` in numpy is used to generate an **identity matrix**. This identity matrix, when scaled by our regularization parameter (often denoted as \\( \\lambda \\) or `r` in our context), is added to the **Gram matrix** \\( X^T X \\). The Gram matrix is the result of multiplying the transpose of matrix \\( X \\) with \\( X \\) itself, capturing the relationships between the features across all data points.\n",
    "\n",
    "By adding the scaled identity matrix to \\( X^T X \\), we incorporate the regularization penalty into our weight calculation process. This penalty ensures that our model doesn't rely too heavily on any single feature and is less prone to overfitting.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Regularization is a powerful technique in machine learning, ensuring our models are robust and perform consistently across both training and unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6226cfa5-0c53-47d5-8b6c-7a5261900e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression_reg(X, y, r=0.0):\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "\n",
    "    XTX = X.T.dot(X)\n",
    "    reg = r * np.eye(XTX.shape[0])\n",
    "    XTX = XTX + reg\n",
    "\n",
    "    XTX_inv = np.linalg.inv(XTX)\n",
    "    w = XTX_inv.dot(X.T).dot(y)\n",
    "    \n",
    "    return w[0], w[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271639a-a720-4855-841a-db65a884cc47",
   "metadata": {},
   "source": [
    "This is a loop for r or regularlization. If r is any of the values within the brackets then the loop will do the following,\n",
    "\n",
    "w_0 , w will run the weights function for the X_Train dataset and y_train, and r wil be equal to the value of r set above. \n",
    "then we will run a linear regression using w_0 the X_val mnultipled by the weight. X_val could be replaced technically by any part of the feature dataset.\n",
    "Then we run the rmse or the mean error so that we can see the margin of error and we print the 2 values, the regularlization and the rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48006689-4f8f-426b-809f-4c6a6110bcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0 0.34085\n",
      " 1e-06 0.34085\n",
      "0.0001 0.34085\n",
      " 0.001 0.34085\n",
      "  0.01 0.34088\n",
      "   0.1 0.34129\n",
      "     1 0.34490\n",
      "     5 0.34774\n",
      "    10 0.34831\n"
     ]
    }
   ],
   "source": [
    "for r in [0, 0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]:\n",
    "    w_0, w = train_linear_regression_reg(X_train, y_train, r=r)\n",
    "    y_pred = w_0 + X_val.dot(w)\n",
    "    rmse_val = rmse(y_val, y_pred)\n",
    "    print('%06s %0.5f' % (r, rmse_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3398c9-ddeb-4a2c-85b7-04df7d8b9271",
   "metadata": {},
   "source": [
    "Is this just the entire code being placed in a function?\n",
    "\n",
    "Not exactly a function, but rather a loop. The loop is iterating over different values of s and, for each value, it's shuffling the data, splitting it into training, validation, and test sets, training a linear regression model, making predictions, and then computing the RMSE. The entire process you previously went through for one dataset split is now being repeated for multiple splits, determined by different random seeds (s).\n",
    "What is s standing for?\n",
    "\n",
    "s stands for the random seed value used by the np.random.seed(s) function. Setting a seed ensures that the random operations, like shuffling in this case, are reproducible. By iterating over different seed values, you're essentially creating different shuffles (or permutations) of your dataset. This allows you to evaluate the model's performance across various data splits, giving a more robust understanding of its generalization capabilities.\n",
    "Think of s as a key to a particular arrangement of a deck of cards. Each key (s value) will shuffle the deck in a unique but reproducible way. By trying out different keys, you're seeing how your game (or model) performs with different card arrangements.\n",
    "Additional Insight:\n",
    "\n",
    "The list rmses is collecting the RMSE values for each shuffle (seed value). At the end of the loop, you'll have an array of RMSE values, one for each seed. This can be useful to compute statistics like the average RMSE or its variance across different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c1661b3-2870-476d-8866-a878ef156c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.33773871600616767\n",
      "1 0.337799935365921\n",
      "2 0.33842870067644554\n",
      "3 0.33200494683066273\n",
      "4 0.33944518625587883\n",
      "5 0.3433819705275054\n",
      "6 0.33853302117632905\n",
      "7 0.34687476972950265\n",
      "8 0.35127368659576985\n",
      "9 0.3341558266506293\n"
     ]
    }
   ],
   "source": [
    "rmses = []\n",
    "\n",
    "for s in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "\n",
    "    n = len(df)\n",
    "\n",
    "    n_val = int(0.2 * n)\n",
    "    n_test = int(0.2 * n)\n",
    "    n_train = n - (n_val + n_test)\n",
    "\n",
    "    idx = np.arange(n)\n",
    "    np.random.seed(s)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    df_shuffled = df.iloc[idx]\n",
    "\n",
    "    df_train = df_shuffled.iloc[:n_train].copy()\n",
    "    df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()\n",
    "    df_test = df_shuffled.iloc[n_train+n_val:].copy()\n",
    "\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    \n",
    "    y_train_orig = df_train.median_house_value.values\n",
    "    y_val_orig = df_val.median_house_value.values\n",
    "    y_test_orig = df_test.median_house_value.values\n",
    "\n",
    "    y_train = np.log1p(y_train_orig)\n",
    "    y_val = np.log1p(y_val_orig)\n",
    "    y_test = np.log1p(y_test_orig)\n",
    "\n",
    "    del df_train['median_house_value']\n",
    "    del df_val['median_house_value']\n",
    "    del df_test['median_house_value']\n",
    "    \n",
    "    X_train = prepare_X(df_train, fillna_value=0)\n",
    "    w_0, w = train_linear_regression(X_train, y_train)\n",
    "\n",
    "    X_val = prepare_X(df_val, fillna_value=0)\n",
    "    y_pred = w_0 + X_val.dot(w)\n",
    "\n",
    "    result = rmse(y_val, y_pred)\n",
    "    print(s, result)\n",
    "    \n",
    "    rmses.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d02ec97c-41f7-4932-806d-67e8695813ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005465718180952141"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5ecaadd-ab52-4f1b-9874-29b6919358c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "\n",
    "n_val = int(0.2 * n)\n",
    "n_test = int(0.2 * n)\n",
    "n_train = n - (n_val + n_test)\n",
    "\n",
    "idx = np.arange(n)\n",
    "np.random.seed(9)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "df_shuffled = df.iloc[idx]\n",
    "\n",
    "df_train = df_shuffled.iloc[:n_train].copy()\n",
    "df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()\n",
    "df_test = df_shuffled.iloc[n_train+n_val:].copy()\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "y_train_orig = df_train.median_house_value.values\n",
    "y_val_orig = df_val.median_house_value.values\n",
    "y_test_orig = df_test.median_house_value.values\n",
    "\n",
    "y_train = np.log1p(y_train_orig)\n",
    "y_val = np.log1p(y_val_orig)\n",
    "y_test = np.log1p(y_test_orig)\n",
    "\n",
    "del df_train['median_house_value']\n",
    "del df_val['median_house_value']\n",
    "del df_test['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b16f81b1-042f-4e41-a1d4-747ca6ea3838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33488435337023326\n"
     ]
    }
   ],
   "source": [
    "X_train = prepare_X(df_train, fillna_value=0)\n",
    "w_0, w = train_linear_regression(X_train, y_train)\n",
    "\n",
    "X_test = prepare_X(df_test, fillna_value=0)\n",
    "y_pred = w_0 + X_test.dot(w)\n",
    "\n",
    "result = rmse(y_test, y_pred)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
